{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Onnx_conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -heel (/home/rtml/.local/lib/python3.6/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -heel (/home/rtml/.local/lib/python3.6/site-packages)\u001b[0m\n",
      "Collecting onnx==1.8.1\n",
      "  Downloading onnx-1.8.1.tar.gz (5.2 MB)\n",
      "     |████████████████████████████████| 5.2 MB 3.8 MB/s            \n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from onnx==1.8.1) (3.19.1)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.6/dist-packages (from onnx==1.8.1) (1.19.5)\n",
      "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.6/dist-packages (from onnx==1.8.1) (3.7.4.3)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from onnx==1.8.1) (1.15.0)\n",
      "Building wheels for collected packages: onnx\n",
      "  Building wheel for onnx (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for onnx: filename=onnx-1.8.1-cp36-cp36m-linux_aarch64.whl size=7039324 sha256=da4a002aac40db3f530a686d841cd6729351871015772ec2e3f7427d83c451ba\n",
      "  Stored in directory: /home/rtml/.cache/pip/wheels/6e/2e/0b/56bf90ac0e0f20a22e645e3e7ca0ab56dcad9b47678694437e\n",
      "Successfully built onnx\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -heel (/home/rtml/.local/lib/python3.6/site-packages)\u001b[0m\n",
      "Installing collected packages: onnx\n",
      "  Attempting uninstall: onnx\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution -heel (/home/rtml/.local/lib/python3.6/site-packages)\u001b[0m\n",
      "    Found existing installation: onnx 1.6.0\n",
      "    Uninstalling onnx-1.6.0:\n",
      "      Successfully uninstalled onnx-1.6.0\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -heel (/home/rtml/.local/lib/python3.6/site-packages)\u001b[0m\n",
      "Successfully installed onnx-1.8.1\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -heel (/home/rtml/.local/lib/python3.6/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -heel (/home/rtml/.local/lib/python3.6/site-packages)\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install onnx==1.8.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import packages\n",
    "import random\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import onnx\n",
    "from efficientnet_pytorch import EfficientNet \n",
    "from torch import nn\n",
    "from torchviz import make_dot\n",
    "import onnxruntime as onnxrt\n",
    "import numpy as np\n",
    "from onnxsim import simplify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.1\n",
      "1.10.0\n"
     ]
    }
   ],
   "source": [
    "print(onnx.__version__)\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "gpuid = 0\n",
    "device = torch.device('cpu') if gpuid < 0 else torch.device(f'cuda:{gpuid}')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking with lift splat shoot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    }
   ],
   "source": [
    "class reduction_5(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate four parameters and assign them as\n",
    "        member parameters.\n",
    "        \"\"\"\n",
    "        super(reduction_5, self).__init__()\n",
    "        self.trunk = EfficientNet.from_pretrained(\"efficientnet-b0\")\n",
    "        self.trunk.set_swish(memory_efficient=False)\n",
    "    \n",
    "    def get_eff_depth(self, x):\n",
    "        #print(\"cam Encode get_eff_depth\")\n",
    "        # adapted from https://github.com/lukemelas/EfficientNet-PyTorch/blob/master/efficientnet_pytorch/model.py#L231\n",
    "        endpoints = dict()\n",
    "        # Stem\n",
    "        x = self.trunk._swish(self.trunk._bn0(self.trunk._conv_stem(x)))\n",
    "        prev_x = x\n",
    "\n",
    "        # Blocks\n",
    "        for idx, block in enumerate(self.trunk._blocks):\n",
    "            drop_connect_rate = self.trunk._global_params.drop_connect_rate\n",
    "            if drop_connect_rate:\n",
    "                drop_connect_rate *= float(idx) / len(self.trunk._blocks) # scale drop connect_rate\n",
    "            x = block(x, drop_connect_rate=drop_connect_rate)\n",
    "            if prev_x.size(2) > x.size(2):\n",
    "                endpoints['reduction_{}'.format(len(endpoints)+1)] = prev_x\n",
    "            prev_x = x\n",
    "\n",
    "        # Head\n",
    "        endpoints['reduction_{}'.format(len(endpoints)+1)] = x\n",
    "        x1 = endpoints['reduction_5']\n",
    "        x2 = endpoints['reduction_4']\n",
    "        return x1\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(\"cam Encode forward\")\n",
    "        x1 = self.get_eff_depth(x)\n",
    "\n",
    "        return x1\n",
    "reduction_5_model = reduction_5()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    }
   ],
   "source": [
    "class reduction_4(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate four parameters and assign them as\n",
    "        member parameters.\n",
    "        \"\"\"\n",
    "        super(reduction_4, self).__init__()\n",
    "        \n",
    "        self.trunk = EfficientNet.from_pretrained(\"efficientnet-b0\")\n",
    "        self.trunk.set_swish(memory_efficient=False)\n",
    "    \n",
    "    def get_eff_depth(self, x):\n",
    "        #print(\"cam Encode get_eff_depth\")\n",
    "        # adapted from https://github.com/lukemelas/EfficientNet-PyTorch/blob/master/efficientnet_pytorch/model.py#L231\n",
    "        endpoints = dict()\n",
    "        # Stem\n",
    "        x = self.trunk._swish(self.trunk._bn0(self.trunk._conv_stem(x)))\n",
    "        prev_x = x\n",
    "\n",
    "        # Blocks\n",
    "        for idx, block in enumerate(self.trunk._blocks):\n",
    "            drop_connect_rate = self.trunk._global_params.drop_connect_rate\n",
    "            if drop_connect_rate:\n",
    "                drop_connect_rate *= float(idx) / len(self.trunk._blocks) # scale drop connect_rate\n",
    "            x = block(x, drop_connect_rate=drop_connect_rate)\n",
    "            if prev_x.size(2) > x.size(2):\n",
    "                endpoints['reduction_{}'.format(len(endpoints)+1)] = prev_x\n",
    "            prev_x = x\n",
    "\n",
    "        # Head\n",
    "        endpoints['reduction_{}'.format(len(endpoints)+1)] = x\n",
    "        x1 = endpoints['reduction_5']\n",
    "        x2 = endpoints['reduction_4']\n",
    "        return x2\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        #print(\"cam Encode forward\")\n",
    "        x4 = self.get_eff_depth(x1)\n",
    "\n",
    "        return x4\n",
    "reduction_4_model = reduction_4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reduction_5(\n",
       "  (trunk): EfficientNet(\n",
       "    (_conv_stem): Conv2dStaticSamePadding(\n",
       "      3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False\n",
       "      (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "    )\n",
       "    (_bn0): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "    (_blocks): ModuleList(\n",
       "      (0): MBConvBlock(\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          32, 32, kernel_size=(3, 3), stride=[1, 1], groups=32, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          32, 8, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          8, 32, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(16, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): Swish()\n",
       "      )\n",
       "      (1): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(96, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          96, 96, kernel_size=(3, 3), stride=[2, 2], groups=96, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(96, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          96, 4, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          4, 96, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): Swish()\n",
       "      )\n",
       "      (2): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          144, 144, kernel_size=(3, 3), stride=(1, 1), groups=144, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): Swish()\n",
       "      )\n",
       "      (3): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          144, 144, kernel_size=(5, 5), stride=[2, 2], groups=144, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(40, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): Swish()\n",
       "      )\n",
       "      (4): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          240, 240, kernel_size=(5, 5), stride=(1, 1), groups=240, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(40, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): Swish()\n",
       "      )\n",
       "      (5): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          240, 240, kernel_size=(3, 3), stride=[2, 2], groups=240, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): Swish()\n",
       "      )\n",
       "      (6): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): Swish()\n",
       "      )\n",
       "      (7): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): Swish()\n",
       "      )\n",
       "      (8): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          480, 480, kernel_size=(5, 5), stride=[1, 1], groups=480, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): Swish()\n",
       "      )\n",
       "      (9): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): Swish()\n",
       "      )\n",
       "      (10): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): Swish()\n",
       "      )\n",
       "      (11): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          672, 672, kernel_size=(5, 5), stride=[2, 2], groups=672, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): Swish()\n",
       "      )\n",
       "      (12): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): Swish()\n",
       "      )\n",
       "      (13): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): Swish()\n",
       "      )\n",
       "      (14): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): Swish()\n",
       "      )\n",
       "      (15): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          1152, 1152, kernel_size=(3, 3), stride=[1, 1], groups=1152, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(320, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): Swish()\n",
       "      )\n",
       "    )\n",
       "    (_conv_head): Conv2dStaticSamePadding(\n",
       "      320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "      (static_padding): Identity()\n",
       "    )\n",
       "    (_bn1): BatchNorm2d(1280, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "    (_avg_pooling): AdaptiveAvgPool2d(output_size=1)\n",
       "    (_dropout): Dropout(p=0.2, inplace=False)\n",
       "    (_fc): Linear(in_features=1280, out_features=1000, bias=True)\n",
       "    (_swish): Swish()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduction_4_model.to(device)\n",
    "reduction_5_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "BATCH_SIZE=1\n",
    "dummy_input1 = torch.ones(6, 3, 128, 352, device= device)\n",
    "print(dummy_input1.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reduction_5(\n",
       "  (trunk): EfficientNet(\n",
       "    (_conv_stem): Conv2dStaticSamePadding(\n",
       "      3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False\n",
       "      (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "    )\n",
       "    (_bn0): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "    (_blocks): ModuleList(\n",
       "      (0): MBConvBlock(\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          32, 32, kernel_size=(3, 3), stride=[1, 1], groups=32, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          32, 8, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          8, 32, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(16, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): Swish()\n",
       "      )\n",
       "      (1): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(96, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          96, 96, kernel_size=(3, 3), stride=[2, 2], groups=96, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(96, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          96, 4, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          4, 96, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): Swish()\n",
       "      )\n",
       "      (2): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          144, 144, kernel_size=(3, 3), stride=(1, 1), groups=144, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): Swish()\n",
       "      )\n",
       "      (3): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          144, 144, kernel_size=(5, 5), stride=[2, 2], groups=144, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(40, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): Swish()\n",
       "      )\n",
       "      (4): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          240, 240, kernel_size=(5, 5), stride=(1, 1), groups=240, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(40, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): Swish()\n",
       "      )\n",
       "      (5): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          240, 240, kernel_size=(3, 3), stride=[2, 2], groups=240, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): Swish()\n",
       "      )\n",
       "      (6): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): Swish()\n",
       "      )\n",
       "      (7): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): Swish()\n",
       "      )\n",
       "      (8): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          480, 480, kernel_size=(5, 5), stride=[1, 1], groups=480, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): Swish()\n",
       "      )\n",
       "      (9): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): Swish()\n",
       "      )\n",
       "      (10): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): Swish()\n",
       "      )\n",
       "      (11): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          672, 672, kernel_size=(5, 5), stride=[2, 2], groups=672, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): Swish()\n",
       "      )\n",
       "      (12): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): Swish()\n",
       "      )\n",
       "      (13): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): Swish()\n",
       "      )\n",
       "      (14): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): Swish()\n",
       "      )\n",
       "      (15): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          1152, 1152, kernel_size=(3, 3), stride=[1, 1], groups=1152, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(320, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): Swish()\n",
       "      )\n",
       "    )\n",
       "    (_conv_head): Conv2dStaticSamePadding(\n",
       "      320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "      (static_padding): Identity()\n",
       "    )\n",
       "    (_bn1): BatchNorm2d(1280, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "    (_avg_pooling): AdaptiveAvgPool2d(output_size=1)\n",
       "    (_dropout): Dropout(p=0.2, inplace=False)\n",
       "    (_fc): Linear(in_features=1280, out_features=1000, bias=True)\n",
       "    (_swish): Swish()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduction_4_model.eval()\n",
    "reduction_5_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Onnx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:26: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph(%input : Float(6, 3, 128, 352, strides=[135168, 45056, 352, 1], requires_grad=0, device=cuda:0),\n",
      "      %trunk._blocks.0._se_reduce.weight : Float(8, 32, 1, 1, strides=[32, 1, 1, 1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.0._se_reduce.bias : Float(8, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.0._se_expand.weight : Float(32, 8, 1, 1, strides=[8, 1, 1, 1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.0._se_expand.bias : Float(32, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.1._se_reduce.weight : Float(4, 96, 1, 1, strides=[96, 1, 1, 1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.1._se_reduce.bias : Float(4, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.1._se_expand.weight : Float(96, 4, 1, 1, strides=[4, 1, 1, 1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.1._se_expand.bias : Float(96, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.2._se_reduce.weight : Float(6, 144, 1, 1, strides=[144, 1, 1, 1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.2._se_reduce.bias : Float(6, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.2._se_expand.weight : Float(144, 6, 1, 1, strides=[6, 1, 1, 1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.2._se_expand.bias : Float(144, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.3._se_reduce.weight : Float(6, 144, 1, 1, strides=[144, 1, 1, 1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.3._se_reduce.bias : Float(6, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.3._se_expand.weight : Float(144, 6, 1, 1, strides=[6, 1, 1, 1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.3._se_expand.bias : Float(144, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.4._se_reduce.weight : Float(10, 240, 1, 1, strides=[240, 1, 1, 1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.4._se_reduce.bias : Float(10, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.4._se_expand.weight : Float(240, 10, 1, 1, strides=[10, 1, 1, 1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.4._se_expand.bias : Float(240, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.5._se_reduce.weight : Float(10, 240, 1, 1, strides=[240, 1, 1, 1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.5._se_reduce.bias : Float(10, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.5._se_expand.weight : Float(240, 10, 1, 1, strides=[10, 1, 1, 1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.5._se_expand.bias : Float(240, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.6._se_reduce.weight : Float(20, 480, 1, 1, strides=[480, 1, 1, 1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.6._se_reduce.bias : Float(20, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.6._se_expand.weight : Float(480, 20, 1, 1, strides=[20, 1, 1, 1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.6._se_expand.bias : Float(480, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.7._se_reduce.weight : Float(20, 480, 1, 1, strides=[480, 1, 1, 1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.7._se_reduce.bias : Float(20, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.7._se_expand.weight : Float(480, 20, 1, 1, strides=[20, 1, 1, 1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.7._se_expand.bias : Float(480, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.8._se_reduce.weight : Float(20, 480, 1, 1, strides=[480, 1, 1, 1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.8._se_reduce.bias : Float(20, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.8._se_expand.weight : Float(480, 20, 1, 1, strides=[20, 1, 1, 1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.8._se_expand.bias : Float(480, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.9._se_reduce.weight : Float(28, 672, 1, 1, strides=[672, 1, 1, 1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.9._se_reduce.bias : Float(28, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.9._se_expand.weight : Float(672, 28, 1, 1, strides=[28, 1, 1, 1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.9._se_expand.bias : Float(672, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.10._se_reduce.weight : Float(28, 672, 1, 1, strides=[672, 1, 1, 1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.10._se_reduce.bias : Float(28, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.10._se_expand.weight : Float(672, 28, 1, 1, strides=[28, 1, 1, 1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.10._se_expand.bias : Float(672, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %568 : Float(32, 3, 3, 3, strides=[27, 9, 3, 1], requires_grad=0, device=cuda:0),\n",
      "      %569 : Float(32, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %571 : Float(32, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=0, device=cuda:0),\n",
      "      %572 : Float(32, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %574 : Float(16, 32, 1, 1, strides=[32, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %575 : Float(16, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %577 : Float(96, 16, 1, 1, strides=[16, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %578 : Float(96, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %580 : Float(96, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=0, device=cuda:0),\n",
      "      %581 : Float(96, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %583 : Float(24, 96, 1, 1, strides=[96, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %584 : Float(24, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %586 : Float(144, 24, 1, 1, strides=[24, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %587 : Float(144, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %589 : Float(144, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=0, device=cuda:0),\n",
      "      %590 : Float(144, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %592 : Float(24, 144, 1, 1, strides=[144, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %593 : Float(24, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %595 : Float(144, 24, 1, 1, strides=[24, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %596 : Float(144, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %598 : Float(144, 1, 5, 5, strides=[25, 25, 5, 1], requires_grad=0, device=cuda:0),\n",
      "      %599 : Float(144, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %601 : Float(40, 144, 1, 1, strides=[144, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %602 : Float(40, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %604 : Float(240, 40, 1, 1, strides=[40, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %605 : Float(240, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %607 : Float(240, 1, 5, 5, strides=[25, 25, 5, 1], requires_grad=0, device=cuda:0),\n",
      "      %608 : Float(240, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %610 : Float(40, 240, 1, 1, strides=[240, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %611 : Float(40, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %613 : Float(240, 40, 1, 1, strides=[40, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %614 : Float(240, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %616 : Float(240, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=0, device=cuda:0),\n",
      "      %617 : Float(240, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %619 : Float(80, 240, 1, 1, strides=[240, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %620 : Float(80, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %622 : Float(480, 80, 1, 1, strides=[80, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %623 : Float(480, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %625 : Float(480, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=0, device=cuda:0),\n",
      "      %626 : Float(480, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %628 : Float(80, 480, 1, 1, strides=[480, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %629 : Float(80, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %631 : Float(480, 80, 1, 1, strides=[80, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %632 : Float(480, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %634 : Float(480, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=0, device=cuda:0),\n",
      "      %635 : Float(480, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %637 : Float(80, 480, 1, 1, strides=[480, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %638 : Float(80, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %640 : Float(480, 80, 1, 1, strides=[80, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %641 : Float(480, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %643 : Float(480, 1, 5, 5, strides=[25, 25, 5, 1], requires_grad=0, device=cuda:0),\n",
      "      %644 : Float(480, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %646 : Float(112, 480, 1, 1, strides=[480, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %647 : Float(112, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %649 : Float(672, 112, 1, 1, strides=[112, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %650 : Float(672, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %652 : Float(672, 1, 5, 5, strides=[25, 25, 5, 1], requires_grad=0, device=cuda:0),\n",
      "      %653 : Float(672, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %655 : Float(112, 672, 1, 1, strides=[672, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %656 : Float(112, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %658 : Float(672, 112, 1, 1, strides=[112, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %659 : Float(672, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %661 : Float(672, 1, 5, 5, strides=[25, 25, 5, 1], requires_grad=0, device=cuda:0),\n",
      "      %662 : Float(672, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %664 : Float(112, 672, 1, 1, strides=[672, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %665 : Float(112, strides=[1], requires_grad=0, device=cuda:0)):\n",
      "  %362 : Float(6, 3, 130, 354, strides=[138060, 46020, 354, 1], requires_grad=0, device=cuda:0) = onnx::Pad[mode=\"constant\", pads=[0, 0, 1, 1, 0, 0, 1, 1], value=0.](%input) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:4174:0\n",
      "  %567 : Float(6, 32, 64, 176, strides=[360448, 11264, 176, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[2, 2]](%362, %568, %569)\n",
      "  %365 : Float(6, 32, 64, 176, strides=[360448, 11264, 176, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%567) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %366 : Float(6, 32, 64, 176, strides=[360448, 11264, 176, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%567, %365) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %367 : Float(6, 32, 66, 178, strides=[375936, 11748, 178, 1], requires_grad=1, device=cuda:0) = onnx::Pad[mode=\"constant\", pads=[0, 0, 1, 1, 0, 0, 1, 1], value=0.](%366) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:4174:0\n",
      "  %570 : Float(6, 32, 64, 176, strides=[360448, 11264, 176, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=32, kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[1, 1]](%367, %571, %572)\n",
      "  %370 : Float(6, 32, 64, 176, strides=[360448, 11264, 176, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%570) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %371 : Float(6, 32, 64, 176, strides=[360448, 11264, 176, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%570, %370) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %372 : Float(6, 32, 1, 1, strides=[32, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::GlobalAveragePool(%371) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1131:0\n",
      "  %373 : Float(6, 8, 1, 1, strides=[8, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%372, %trunk._blocks.0._se_reduce.weight, %trunk._blocks.0._se_reduce.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
      "  %374 : Float(6, 8, 1, 1, strides=[8, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%373) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %375 : Float(6, 8, 1, 1, strides=[8, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%373, %374) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %376 : Float(6, 32, 1, 1, strides=[32, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%375, %trunk._blocks.0._se_expand.weight, %trunk._blocks.0._se_expand.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
      "  %377 : Float(6, 32, 1, 1, strides=[32, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%376) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
      "  %378 : Float(6, 32, 64, 176, strides=[360448, 11264, 176, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%377, %371) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
      "  %573 : Float(6, 16, 64, 176, strides=[180224, 11264, 176, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%378, %574, %575)\n",
      "  %576 : Float(6, 96, 64, 176, strides=[1081344, 11264, 176, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%573, %577, %578)\n",
      "  %383 : Float(6, 96, 64, 176, strides=[1081344, 11264, 176, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%576) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %384 : Float(6, 96, 64, 176, strides=[1081344, 11264, 176, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%576, %383) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %385 : Float(6, 96, 66, 178, strides=[1127808, 11748, 178, 1], requires_grad=1, device=cuda:0) = onnx::Pad[mode=\"constant\", pads=[0, 0, 1, 1, 0, 0, 1, 1], value=0.](%384) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:4174:0\n",
      "  %579 : Float(6, 96, 32, 88, strides=[270336, 2816, 88, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=96, kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[2, 2]](%385, %580, %581)\n",
      "  %388 : Float(6, 96, 32, 88, strides=[270336, 2816, 88, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%579) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %389 : Float(6, 96, 32, 88, strides=[270336, 2816, 88, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%579, %388) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %390 : Float(6, 96, 1, 1, strides=[96, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::GlobalAveragePool(%389) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1131:0\n",
      "  %391 : Float(6, 4, 1, 1, strides=[4, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%390, %trunk._blocks.1._se_reduce.weight, %trunk._blocks.1._se_reduce.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
      "  %392 : Float(6, 4, 1, 1, strides=[4, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%391) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %393 : Float(6, 4, 1, 1, strides=[4, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%391, %392) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %394 : Float(6, 96, 1, 1, strides=[96, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%393, %trunk._blocks.1._se_expand.weight, %trunk._blocks.1._se_expand.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
      "  %395 : Float(6, 96, 1, 1, strides=[96, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%394) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
      "  %396 : Float(6, 96, 32, 88, strides=[270336, 2816, 88, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%395, %389) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
      "  %582 : Float(6, 24, 32, 88, strides=[67584, 2816, 88, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%396, %583, %584)\n",
      "  %585 : Float(6, 144, 32, 88, strides=[405504, 2816, 88, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%582, %586, %587)\n",
      "  %401 : Float(6, 144, 32, 88, strides=[405504, 2816, 88, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%585) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %402 : Float(6, 144, 32, 88, strides=[405504, 2816, 88, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%585, %401) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %403 : Float(6, 144, 34, 90, strides=[440640, 3060, 90, 1], requires_grad=1, device=cuda:0) = onnx::Pad[mode=\"constant\", pads=[0, 0, 1, 1, 0, 0, 1, 1], value=0.](%402) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:4174:0\n",
      "  %588 : Float(6, 144, 32, 88, strides=[405504, 2816, 88, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=144, kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[1, 1]](%403, %589, %590)\n",
      "  %406 : Float(6, 144, 32, 88, strides=[405504, 2816, 88, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%588) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %407 : Float(6, 144, 32, 88, strides=[405504, 2816, 88, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%588, %406) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %408 : Float(6, 144, 1, 1, strides=[144, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::GlobalAveragePool(%407) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1131:0\n",
      "  %409 : Float(6, 6, 1, 1, strides=[6, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%408, %trunk._blocks.2._se_reduce.weight, %trunk._blocks.2._se_reduce.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
      "  %410 : Float(6, 6, 1, 1, strides=[6, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%409) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %411 : Float(6, 6, 1, 1, strides=[6, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%409, %410) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %412 : Float(6, 144, 1, 1, strides=[144, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%411, %trunk._blocks.2._se_expand.weight, %trunk._blocks.2._se_expand.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
      "  %413 : Float(6, 144, 1, 1, strides=[144, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%412) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
      "  %414 : Float(6, 144, 32, 88, strides=[405504, 2816, 88, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%413, %407) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
      "  %591 : Float(6, 24, 32, 88, strides=[67584, 2816, 88, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%414, %592, %593)\n",
      "  %417 : Float(6, 24, 32, 88, strides=[67584, 2816, 88, 1], requires_grad=1, device=cuda:0) = onnx::Add(%591, %582) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:131:0\n",
      "  %594 : Float(6, 144, 32, 88, strides=[405504, 2816, 88, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%417, %595, %596)\n",
      "  %420 : Float(6, 144, 32, 88, strides=[405504, 2816, 88, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%594) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %421 : Float(6, 144, 32, 88, strides=[405504, 2816, 88, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%594, %420) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %422 : Float(6, 144, 36, 92, strides=[476928, 3312, 92, 1], requires_grad=1, device=cuda:0) = onnx::Pad[mode=\"constant\", pads=[0, 0, 2, 2, 0, 0, 2, 2], value=0.](%421) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:4174:0\n",
      "  %597 : Float(6, 144, 16, 44, strides=[101376, 704, 44, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=144, kernel_shape=[5, 5], pads=[0, 0, 0, 0], strides=[2, 2]](%422, %598, %599)\n",
      "  %425 : Float(6, 144, 16, 44, strides=[101376, 704, 44, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%597) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %426 : Float(6, 144, 16, 44, strides=[101376, 704, 44, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%597, %425) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %427 : Float(6, 144, 1, 1, strides=[144, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::GlobalAveragePool(%426) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1131:0\n",
      "  %428 : Float(6, 6, 1, 1, strides=[6, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%427, %trunk._blocks.3._se_reduce.weight, %trunk._blocks.3._se_reduce.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
      "  %429 : Float(6, 6, 1, 1, strides=[6, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%428) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %430 : Float(6, 6, 1, 1, strides=[6, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%428, %429) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %431 : Float(6, 144, 1, 1, strides=[144, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%430, %trunk._blocks.3._se_expand.weight, %trunk._blocks.3._se_expand.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
      "  %432 : Float(6, 144, 1, 1, strides=[144, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%431) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
      "  %433 : Float(6, 144, 16, 44, strides=[101376, 704, 44, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%432, %426) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
      "  %600 : Float(6, 40, 16, 44, strides=[28160, 704, 44, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%433, %601, %602)\n",
      "  %603 : Float(6, 240, 16, 44, strides=[168960, 704, 44, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%600, %604, %605)\n",
      "  %438 : Float(6, 240, 16, 44, strides=[168960, 704, 44, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%603) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %439 : Float(6, 240, 16, 44, strides=[168960, 704, 44, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%603, %438) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %440 : Float(6, 240, 20, 48, strides=[230400, 960, 48, 1], requires_grad=1, device=cuda:0) = onnx::Pad[mode=\"constant\", pads=[0, 0, 2, 2, 0, 0, 2, 2], value=0.](%439) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:4174:0\n",
      "  %606 : Float(6, 240, 16, 44, strides=[168960, 704, 44, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=240, kernel_shape=[5, 5], pads=[0, 0, 0, 0], strides=[1, 1]](%440, %607, %608)\n",
      "  %443 : Float(6, 240, 16, 44, strides=[168960, 704, 44, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%606) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %444 : Float(6, 240, 16, 44, strides=[168960, 704, 44, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%606, %443) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %445 : Float(6, 240, 1, 1, strides=[240, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::GlobalAveragePool(%444) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1131:0\n",
      "  %446 : Float(6, 10, 1, 1, strides=[10, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%445, %trunk._blocks.4._se_reduce.weight, %trunk._blocks.4._se_reduce.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
      "  %447 : Float(6, 10, 1, 1, strides=[10, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%446) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %448 : Float(6, 10, 1, 1, strides=[10, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%446, %447) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %449 : Float(6, 240, 1, 1, strides=[240, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%448, %trunk._blocks.4._se_expand.weight, %trunk._blocks.4._se_expand.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
      "  %450 : Float(6, 240, 1, 1, strides=[240, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%449) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
      "  %451 : Float(6, 240, 16, 44, strides=[168960, 704, 44, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%450, %444) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
      "  %609 : Float(6, 40, 16, 44, strides=[28160, 704, 44, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%451, %610, %611)\n",
      "  %454 : Float(6, 40, 16, 44, strides=[28160, 704, 44, 1], requires_grad=1, device=cuda:0) = onnx::Add(%609, %600) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:131:0\n",
      "  %612 : Float(6, 240, 16, 44, strides=[168960, 704, 44, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%454, %613, %614)\n",
      "  %457 : Float(6, 240, 16, 44, strides=[168960, 704, 44, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%612) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %458 : Float(6, 240, 16, 44, strides=[168960, 704, 44, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%612, %457) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %459 : Float(6, 240, 18, 46, strides=[198720, 828, 46, 1], requires_grad=1, device=cuda:0) = onnx::Pad[mode=\"constant\", pads=[0, 0, 1, 1, 0, 0, 1, 1], value=0.](%458) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:4174:0\n",
      "  %615 : Float(6, 240, 8, 22, strides=[42240, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=240, kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[2, 2]](%459, %616, %617)\n",
      "  %462 : Float(6, 240, 8, 22, strides=[42240, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%615) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %463 : Float(6, 240, 8, 22, strides=[42240, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%615, %462) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %464 : Float(6, 240, 1, 1, strides=[240, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::GlobalAveragePool(%463) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1131:0\n",
      "  %465 : Float(6, 10, 1, 1, strides=[10, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%464, %trunk._blocks.5._se_reduce.weight, %trunk._blocks.5._se_reduce.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
      "  %466 : Float(6, 10, 1, 1, strides=[10, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%465) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %467 : Float(6, 10, 1, 1, strides=[10, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%465, %466) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %468 : Float(6, 240, 1, 1, strides=[240, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%467, %trunk._blocks.5._se_expand.weight, %trunk._blocks.5._se_expand.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
      "  %469 : Float(6, 240, 1, 1, strides=[240, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%468) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
      "  %470 : Float(6, 240, 8, 22, strides=[42240, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%469, %463) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
      "  %618 : Float(6, 80, 8, 22, strides=[14080, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%470, %619, %620)\n",
      "  %621 : Float(6, 480, 8, 22, strides=[84480, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%618, %622, %623)\n",
      "  %475 : Float(6, 480, 8, 22, strides=[84480, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%621) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %476 : Float(6, 480, 8, 22, strides=[84480, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%621, %475) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %477 : Float(6, 480, 10, 24, strides=[115200, 240, 24, 1], requires_grad=1, device=cuda:0) = onnx::Pad[mode=\"constant\", pads=[0, 0, 1, 1, 0, 0, 1, 1], value=0.](%476) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:4174:0\n",
      "  %624 : Float(6, 480, 8, 22, strides=[84480, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=480, kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[1, 1]](%477, %625, %626)\n",
      "  %480 : Float(6, 480, 8, 22, strides=[84480, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%624) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %481 : Float(6, 480, 8, 22, strides=[84480, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%624, %480) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %482 : Float(6, 480, 1, 1, strides=[480, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::GlobalAveragePool(%481) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1131:0\n",
      "  %483 : Float(6, 20, 1, 1, strides=[20, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%482, %trunk._blocks.6._se_reduce.weight, %trunk._blocks.6._se_reduce.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
      "  %484 : Float(6, 20, 1, 1, strides=[20, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%483) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %485 : Float(6, 20, 1, 1, strides=[20, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%483, %484) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %486 : Float(6, 480, 1, 1, strides=[480, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%485, %trunk._blocks.6._se_expand.weight, %trunk._blocks.6._se_expand.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
      "  %487 : Float(6, 480, 1, 1, strides=[480, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%486) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
      "  %488 : Float(6, 480, 8, 22, strides=[84480, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%487, %481) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
      "  %627 : Float(6, 80, 8, 22, strides=[14080, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%488, %628, %629)\n",
      "  %491 : Float(6, 80, 8, 22, strides=[14080, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Add(%627, %618) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:131:0\n",
      "  %630 : Float(6, 480, 8, 22, strides=[84480, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%491, %631, %632)\n",
      "  %494 : Float(6, 480, 8, 22, strides=[84480, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%630) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %495 : Float(6, 480, 8, 22, strides=[84480, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%630, %494) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %496 : Float(6, 480, 10, 24, strides=[115200, 240, 24, 1], requires_grad=1, device=cuda:0) = onnx::Pad[mode=\"constant\", pads=[0, 0, 1, 1, 0, 0, 1, 1], value=0.](%495) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:4174:0\n",
      "  %633 : Float(6, 480, 8, 22, strides=[84480, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=480, kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[1, 1]](%496, %634, %635)\n",
      "  %499 : Float(6, 480, 8, 22, strides=[84480, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%633) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %500 : Float(6, 480, 8, 22, strides=[84480, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%633, %499) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %501 : Float(6, 480, 1, 1, strides=[480, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::GlobalAveragePool(%500) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1131:0\n",
      "  %502 : Float(6, 20, 1, 1, strides=[20, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%501, %trunk._blocks.7._se_reduce.weight, %trunk._blocks.7._se_reduce.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
      "  %503 : Float(6, 20, 1, 1, strides=[20, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%502) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %504 : Float(6, 20, 1, 1, strides=[20, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%502, %503) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %505 : Float(6, 480, 1, 1, strides=[480, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%504, %trunk._blocks.7._se_expand.weight, %trunk._blocks.7._se_expand.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
      "  %506 : Float(6, 480, 1, 1, strides=[480, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%505) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
      "  %507 : Float(6, 480, 8, 22, strides=[84480, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%506, %500) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
      "  %636 : Float(6, 80, 8, 22, strides=[14080, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%507, %637, %638)\n",
      "  %510 : Float(6, 80, 8, 22, strides=[14080, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Add(%636, %491) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:131:0\n",
      "  %639 : Float(6, 480, 8, 22, strides=[84480, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%510, %640, %641)\n",
      "  %513 : Float(6, 480, 8, 22, strides=[84480, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%639) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %514 : Float(6, 480, 8, 22, strides=[84480, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%639, %513) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %515 : Float(6, 480, 12, 26, strides=[149760, 312, 26, 1], requires_grad=1, device=cuda:0) = onnx::Pad[mode=\"constant\", pads=[0, 0, 2, 2, 0, 0, 2, 2], value=0.](%514) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:4174:0\n",
      "  %642 : Float(6, 480, 8, 22, strides=[84480, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=480, kernel_shape=[5, 5], pads=[0, 0, 0, 0], strides=[1, 1]](%515, %643, %644)\n",
      "  %518 : Float(6, 480, 8, 22, strides=[84480, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%642) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %519 : Float(6, 480, 8, 22, strides=[84480, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%642, %518) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %520 : Float(6, 480, 1, 1, strides=[480, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::GlobalAveragePool(%519) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1131:0\n",
      "  %521 : Float(6, 20, 1, 1, strides=[20, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%520, %trunk._blocks.8._se_reduce.weight, %trunk._blocks.8._se_reduce.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
      "  %522 : Float(6, 20, 1, 1, strides=[20, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%521) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %523 : Float(6, 20, 1, 1, strides=[20, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%521, %522) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %524 : Float(6, 480, 1, 1, strides=[480, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%523, %trunk._blocks.8._se_expand.weight, %trunk._blocks.8._se_expand.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
      "  %525 : Float(6, 480, 1, 1, strides=[480, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%524) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
      "  %526 : Float(6, 480, 8, 22, strides=[84480, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%525, %519) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
      "  %645 : Float(6, 112, 8, 22, strides=[19712, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%526, %646, %647)\n",
      "  %648 : Float(6, 672, 8, 22, strides=[118272, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%645, %649, %650)\n",
      "  %531 : Float(6, 672, 8, 22, strides=[118272, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%648) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %532 : Float(6, 672, 8, 22, strides=[118272, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%648, %531) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %533 : Float(6, 672, 12, 26, strides=[209664, 312, 26, 1], requires_grad=1, device=cuda:0) = onnx::Pad[mode=\"constant\", pads=[0, 0, 2, 2, 0, 0, 2, 2], value=0.](%532) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:4174:0\n",
      "  %651 : Float(6, 672, 8, 22, strides=[118272, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=672, kernel_shape=[5, 5], pads=[0, 0, 0, 0], strides=[1, 1]](%533, %652, %653)\n",
      "  %536 : Float(6, 672, 8, 22, strides=[118272, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%651) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %537 : Float(6, 672, 8, 22, strides=[118272, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%651, %536) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %538 : Float(6, 672, 1, 1, strides=[672, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::GlobalAveragePool(%537) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1131:0\n",
      "  %539 : Float(6, 28, 1, 1, strides=[28, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%538, %trunk._blocks.9._se_reduce.weight, %trunk._blocks.9._se_reduce.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
      "  %540 : Float(6, 28, 1, 1, strides=[28, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%539) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %541 : Float(6, 28, 1, 1, strides=[28, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%539, %540) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %542 : Float(6, 672, 1, 1, strides=[672, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%541, %trunk._blocks.9._se_expand.weight, %trunk._blocks.9._se_expand.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
      "  %543 : Float(6, 672, 1, 1, strides=[672, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%542) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
      "  %544 : Float(6, 672, 8, 22, strides=[118272, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%543, %537) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
      "  %654 : Float(6, 112, 8, 22, strides=[19712, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%544, %655, %656)\n",
      "  %547 : Float(6, 112, 8, 22, strides=[19712, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Add(%654, %645) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:131:0\n",
      "  %657 : Float(6, 672, 8, 22, strides=[118272, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%547, %658, %659)\n",
      "  %550 : Float(6, 672, 8, 22, strides=[118272, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%657) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %551 : Float(6, 672, 8, 22, strides=[118272, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%657, %550) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %552 : Float(6, 672, 12, 26, strides=[209664, 312, 26, 1], requires_grad=1, device=cuda:0) = onnx::Pad[mode=\"constant\", pads=[0, 0, 2, 2, 0, 0, 2, 2], value=0.](%551) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:4174:0\n",
      "  %660 : Float(6, 672, 8, 22, strides=[118272, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=672, kernel_shape=[5, 5], pads=[0, 0, 0, 0], strides=[1, 1]](%552, %661, %662)\n",
      "  %555 : Float(6, 672, 8, 22, strides=[118272, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%660) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %556 : Float(6, 672, 8, 22, strides=[118272, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%660, %555) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %557 : Float(6, 672, 1, 1, strides=[672, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::GlobalAveragePool(%556) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1131:0\n",
      "  %558 : Float(6, 28, 1, 1, strides=[28, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%557, %trunk._blocks.10._se_reduce.weight, %trunk._blocks.10._se_reduce.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
      "  %559 : Float(6, 28, 1, 1, strides=[28, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%558) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %560 : Float(6, 28, 1, 1, strides=[28, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%558, %559) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %561 : Float(6, 672, 1, 1, strides=[672, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%560, %trunk._blocks.10._se_expand.weight, %trunk._blocks.10._se_expand.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
      "  %562 : Float(6, 672, 1, 1, strides=[672, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%561) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
      "  %563 : Float(6, 672, 8, 22, strides=[118272, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%562, %556) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
      "  %663 : Float(6, 112, 8, 22, strides=[19712, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%563, %664, %665)\n",
      "  %output : Float(6, 112, 8, 22, strides=[19712, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Add(%663, %547) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:131:0\n",
      "  return (%output)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ONNX_PATH='models/reduction_4_model.onnx'\n",
    "torch.onnx.export(reduction_4_model, (dummy_input1,dummy_input2), ONNX_PATH,\n",
    "                    export_params=True,\n",
    "                    verbose=True, \n",
    "                    input_names=['input'],\n",
    "                    output_names=['output'],\n",
    "\n",
    "                    opset_version=10,\n",
    "                    do_constant_folding=False,\n",
    "                    #example_outputs=torch_out,\n",
    "                    #strip_doc_string=True,\n",
    "                    #dynamic_axes=None,\n",
    "                    keep_initializers_as_inputs=True\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:25: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph(%input : Float(6, 3, 128, 352, strides=[135168, 45056, 352, 1], requires_grad=0, device=cuda:0),\n",
      "      %trunk._blocks.0._se_reduce.weight : Float(8, 32, 1, 1, strides=[32, 1, 1, 1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.0._se_reduce.bias : Float(8, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.0._se_expand.weight : Float(32, 8, 1, 1, strides=[8, 1, 1, 1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.0._se_expand.bias : Float(32, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.1._se_reduce.weight : Float(4, 96, 1, 1, strides=[96, 1, 1, 1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.1._se_reduce.bias : Float(4, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.1._se_expand.weight : Float(96, 4, 1, 1, strides=[4, 1, 1, 1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.1._se_expand.bias : Float(96, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.2._se_reduce.weight : Float(6, 144, 1, 1, strides=[144, 1, 1, 1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.2._se_reduce.bias : Float(6, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.2._se_expand.weight : Float(144, 6, 1, 1, strides=[6, 1, 1, 1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.2._se_expand.bias : Float(144, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.3._se_reduce.weight : Float(6, 144, 1, 1, strides=[144, 1, 1, 1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.3._se_reduce.bias : Float(6, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.3._se_expand.weight : Float(144, 6, 1, 1, strides=[6, 1, 1, 1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.3._se_expand.bias : Float(144, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.4._se_reduce.weight : Float(10, 240, 1, 1, strides=[240, 1, 1, 1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.4._se_reduce.bias : Float(10, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.4._se_expand.weight : Float(240, 10, 1, 1, strides=[10, 1, 1, 1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.4._se_expand.bias : Float(240, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.5._se_reduce.weight : Float(10, 240, 1, 1, strides=[240, 1, 1, 1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.5._se_reduce.bias : Float(10, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.5._se_expand.weight : Float(240, 10, 1, 1, strides=[10, 1, 1, 1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.5._se_expand.bias : Float(240, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.6._se_reduce.weight : Float(20, 480, 1, 1, strides=[480, 1, 1, 1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.6._se_reduce.bias : Float(20, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.6._se_expand.weight : Float(480, 20, 1, 1, strides=[20, 1, 1, 1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.6._se_expand.bias : Float(480, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.7._se_reduce.weight : Float(20, 480, 1, 1, strides=[480, 1, 1, 1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.7._se_reduce.bias : Float(20, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.7._se_expand.weight : Float(480, 20, 1, 1, strides=[20, 1, 1, 1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.7._se_expand.bias : Float(480, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.8._se_reduce.weight : Float(20, 480, 1, 1, strides=[480, 1, 1, 1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.8._se_reduce.bias : Float(20, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.8._se_expand.weight : Float(480, 20, 1, 1, strides=[20, 1, 1, 1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.8._se_expand.bias : Float(480, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.9._se_reduce.weight : Float(28, 672, 1, 1, strides=[672, 1, 1, 1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.9._se_reduce.bias : Float(28, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.9._se_expand.weight : Float(672, 28, 1, 1, strides=[28, 1, 1, 1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.9._se_expand.bias : Float(672, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.10._se_reduce.weight : Float(28, 672, 1, 1, strides=[672, 1, 1, 1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.10._se_reduce.bias : Float(28, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.10._se_expand.weight : Float(672, 28, 1, 1, strides=[28, 1, 1, 1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.10._se_expand.bias : Float(672, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.11._se_reduce.weight : Float(28, 672, 1, 1, strides=[672, 1, 1, 1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.11._se_reduce.bias : Float(28, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.11._se_expand.weight : Float(672, 28, 1, 1, strides=[28, 1, 1, 1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.11._se_expand.bias : Float(672, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.12._se_reduce.weight : Float(48, 1152, 1, 1, strides=[1152, 1, 1, 1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.12._se_reduce.bias : Float(48, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.12._se_expand.weight : Float(1152, 48, 1, 1, strides=[48, 1, 1, 1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.12._se_expand.bias : Float(1152, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.13._se_reduce.weight : Float(48, 1152, 1, 1, strides=[1152, 1, 1, 1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.13._se_reduce.bias : Float(48, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.13._se_expand.weight : Float(1152, 48, 1, 1, strides=[48, 1, 1, 1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.13._se_expand.bias : Float(1152, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.14._se_reduce.weight : Float(48, 1152, 1, 1, strides=[1152, 1, 1, 1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.14._se_reduce.bias : Float(48, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.14._se_expand.weight : Float(1152, 48, 1, 1, strides=[48, 1, 1, 1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.14._se_expand.bias : Float(1152, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.15._se_reduce.weight : Float(48, 1152, 1, 1, strides=[1152, 1, 1, 1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.15._se_reduce.bias : Float(48, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.15._se_expand.weight : Float(1152, 48, 1, 1, strides=[48, 1, 1, 1], requires_grad=1, device=cuda:0),\n",
      "      %trunk._blocks.15._se_expand.bias : Float(1152, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %660 : Float(32, 3, 3, 3, strides=[27, 9, 3, 1], requires_grad=0, device=cuda:0),\n",
      "      %661 : Float(32, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %663 : Float(32, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=0, device=cuda:0),\n",
      "      %664 : Float(32, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %666 : Float(16, 32, 1, 1, strides=[32, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %667 : Float(16, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %669 : Float(96, 16, 1, 1, strides=[16, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %670 : Float(96, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %672 : Float(96, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=0, device=cuda:0),\n",
      "      %673 : Float(96, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %675 : Float(24, 96, 1, 1, strides=[96, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %676 : Float(24, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %678 : Float(144, 24, 1, 1, strides=[24, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %679 : Float(144, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %681 : Float(144, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=0, device=cuda:0),\n",
      "      %682 : Float(144, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %684 : Float(24, 144, 1, 1, strides=[144, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %685 : Float(24, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %687 : Float(144, 24, 1, 1, strides=[24, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %688 : Float(144, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %690 : Float(144, 1, 5, 5, strides=[25, 25, 5, 1], requires_grad=0, device=cuda:0),\n",
      "      %691 : Float(144, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %693 : Float(40, 144, 1, 1, strides=[144, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %694 : Float(40, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %696 : Float(240, 40, 1, 1, strides=[40, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %697 : Float(240, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %699 : Float(240, 1, 5, 5, strides=[25, 25, 5, 1], requires_grad=0, device=cuda:0),\n",
      "      %700 : Float(240, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %702 : Float(40, 240, 1, 1, strides=[240, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %703 : Float(40, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %705 : Float(240, 40, 1, 1, strides=[40, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %706 : Float(240, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %708 : Float(240, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=0, device=cuda:0),\n",
      "      %709 : Float(240, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %711 : Float(80, 240, 1, 1, strides=[240, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %712 : Float(80, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %714 : Float(480, 80, 1, 1, strides=[80, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %715 : Float(480, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %717 : Float(480, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=0, device=cuda:0),\n",
      "      %718 : Float(480, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %720 : Float(80, 480, 1, 1, strides=[480, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %721 : Float(80, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %723 : Float(480, 80, 1, 1, strides=[80, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %724 : Float(480, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %726 : Float(480, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=0, device=cuda:0),\n",
      "      %727 : Float(480, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %729 : Float(80, 480, 1, 1, strides=[480, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %730 : Float(80, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %732 : Float(480, 80, 1, 1, strides=[80, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %733 : Float(480, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %735 : Float(480, 1, 5, 5, strides=[25, 25, 5, 1], requires_grad=0, device=cuda:0),\n",
      "      %736 : Float(480, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %738 : Float(112, 480, 1, 1, strides=[480, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %739 : Float(112, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %741 : Float(672, 112, 1, 1, strides=[112, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %742 : Float(672, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %744 : Float(672, 1, 5, 5, strides=[25, 25, 5, 1], requires_grad=0, device=cuda:0),\n",
      "      %745 : Float(672, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %747 : Float(112, 672, 1, 1, strides=[672, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %748 : Float(112, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %750 : Float(672, 112, 1, 1, strides=[112, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %751 : Float(672, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %753 : Float(672, 1, 5, 5, strides=[25, 25, 5, 1], requires_grad=0, device=cuda:0),\n",
      "      %754 : Float(672, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %756 : Float(112, 672, 1, 1, strides=[672, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %757 : Float(112, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %759 : Float(672, 112, 1, 1, strides=[112, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %760 : Float(672, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %762 : Float(672, 1, 5, 5, strides=[25, 25, 5, 1], requires_grad=0, device=cuda:0),\n",
      "      %763 : Float(672, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %765 : Float(192, 672, 1, 1, strides=[672, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %766 : Float(192, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %768 : Float(1152, 192, 1, 1, strides=[192, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %769 : Float(1152, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %771 : Float(1152, 1, 5, 5, strides=[25, 25, 5, 1], requires_grad=0, device=cuda:0),\n",
      "      %772 : Float(1152, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %774 : Float(192, 1152, 1, 1, strides=[1152, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %775 : Float(192, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %777 : Float(1152, 192, 1, 1, strides=[192, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %778 : Float(1152, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %780 : Float(1152, 1, 5, 5, strides=[25, 25, 5, 1], requires_grad=0, device=cuda:0),\n",
      "      %781 : Float(1152, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %783 : Float(192, 1152, 1, 1, strides=[1152, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %784 : Float(192, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %786 : Float(1152, 192, 1, 1, strides=[192, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %787 : Float(1152, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %789 : Float(1152, 1, 5, 5, strides=[25, 25, 5, 1], requires_grad=0, device=cuda:0),\n",
      "      %790 : Float(1152, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %792 : Float(192, 1152, 1, 1, strides=[1152, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %793 : Float(192, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %795 : Float(1152, 192, 1, 1, strides=[192, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %796 : Float(1152, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %798 : Float(1152, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=0, device=cuda:0),\n",
      "      %799 : Float(1152, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %801 : Float(320, 1152, 1, 1, strides=[1152, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %802 : Float(320, strides=[1], requires_grad=0, device=cuda:0)):\n",
      "  %361 : Float(6, 3, 130, 354, strides=[138060, 46020, 354, 1], requires_grad=0, device=cuda:0) = onnx::Pad[mode=\"constant\", pads=[0, 0, 1, 1, 0, 0, 1, 1], value=0.](%input) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:4174:0\n",
      "  %659 : Float(6, 32, 64, 176, strides=[360448, 11264, 176, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[2, 2]](%361, %660, %661)\n",
      "  %364 : Float(6, 32, 64, 176, strides=[360448, 11264, 176, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%659) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %365 : Float(6, 32, 64, 176, strides=[360448, 11264, 176, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%659, %364) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %366 : Float(6, 32, 66, 178, strides=[375936, 11748, 178, 1], requires_grad=1, device=cuda:0) = onnx::Pad[mode=\"constant\", pads=[0, 0, 1, 1, 0, 0, 1, 1], value=0.](%365) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:4174:0\n",
      "  %662 : Float(6, 32, 64, 176, strides=[360448, 11264, 176, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=32, kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[1, 1]](%366, %663, %664)\n",
      "  %369 : Float(6, 32, 64, 176, strides=[360448, 11264, 176, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%662) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %370 : Float(6, 32, 64, 176, strides=[360448, 11264, 176, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%662, %369) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %371 : Float(6, 32, 1, 1, strides=[32, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::GlobalAveragePool(%370) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1131:0\n",
      "  %372 : Float(6, 8, 1, 1, strides=[8, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%371, %trunk._blocks.0._se_reduce.weight, %trunk._blocks.0._se_reduce.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
      "  %373 : Float(6, 8, 1, 1, strides=[8, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%372) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %374 : Float(6, 8, 1, 1, strides=[8, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%372, %373) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %375 : Float(6, 32, 1, 1, strides=[32, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%374, %trunk._blocks.0._se_expand.weight, %trunk._blocks.0._se_expand.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
      "  %376 : Float(6, 32, 1, 1, strides=[32, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%375) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
      "  %377 : Float(6, 32, 64, 176, strides=[360448, 11264, 176, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%376, %370) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
      "  %665 : Float(6, 16, 64, 176, strides=[180224, 11264, 176, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%377, %666, %667)\n",
      "  %668 : Float(6, 96, 64, 176, strides=[1081344, 11264, 176, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%665, %669, %670)\n",
      "  %382 : Float(6, 96, 64, 176, strides=[1081344, 11264, 176, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%668) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %383 : Float(6, 96, 64, 176, strides=[1081344, 11264, 176, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%668, %382) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %384 : Float(6, 96, 66, 178, strides=[1127808, 11748, 178, 1], requires_grad=1, device=cuda:0) = onnx::Pad[mode=\"constant\", pads=[0, 0, 1, 1, 0, 0, 1, 1], value=0.](%383) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:4174:0\n",
      "  %671 : Float(6, 96, 32, 88, strides=[270336, 2816, 88, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=96, kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[2, 2]](%384, %672, %673)\n",
      "  %387 : Float(6, 96, 32, 88, strides=[270336, 2816, 88, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%671) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %388 : Float(6, 96, 32, 88, strides=[270336, 2816, 88, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%671, %387) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %389 : Float(6, 96, 1, 1, strides=[96, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::GlobalAveragePool(%388) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1131:0\n",
      "  %390 : Float(6, 4, 1, 1, strides=[4, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%389, %trunk._blocks.1._se_reduce.weight, %trunk._blocks.1._se_reduce.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
      "  %391 : Float(6, 4, 1, 1, strides=[4, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%390) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %392 : Float(6, 4, 1, 1, strides=[4, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%390, %391) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %393 : Float(6, 96, 1, 1, strides=[96, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%392, %trunk._blocks.1._se_expand.weight, %trunk._blocks.1._se_expand.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
      "  %394 : Float(6, 96, 1, 1, strides=[96, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%393) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
      "  %395 : Float(6, 96, 32, 88, strides=[270336, 2816, 88, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%394, %388) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
      "  %674 : Float(6, 24, 32, 88, strides=[67584, 2816, 88, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%395, %675, %676)\n",
      "  %677 : Float(6, 144, 32, 88, strides=[405504, 2816, 88, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%674, %678, %679)\n",
      "  %400 : Float(6, 144, 32, 88, strides=[405504, 2816, 88, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%677) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %401 : Float(6, 144, 32, 88, strides=[405504, 2816, 88, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%677, %400) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %402 : Float(6, 144, 34, 90, strides=[440640, 3060, 90, 1], requires_grad=1, device=cuda:0) = onnx::Pad[mode=\"constant\", pads=[0, 0, 1, 1, 0, 0, 1, 1], value=0.](%401) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:4174:0\n",
      "  %680 : Float(6, 144, 32, 88, strides=[405504, 2816, 88, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=144, kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[1, 1]](%402, %681, %682)\n",
      "  %405 : Float(6, 144, 32, 88, strides=[405504, 2816, 88, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%680) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %406 : Float(6, 144, 32, 88, strides=[405504, 2816, 88, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%680, %405) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %407 : Float(6, 144, 1, 1, strides=[144, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::GlobalAveragePool(%406) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1131:0\n",
      "  %408 : Float(6, 6, 1, 1, strides=[6, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%407, %trunk._blocks.2._se_reduce.weight, %trunk._blocks.2._se_reduce.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
      "  %409 : Float(6, 6, 1, 1, strides=[6, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%408) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %410 : Float(6, 6, 1, 1, strides=[6, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%408, %409) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %411 : Float(6, 144, 1, 1, strides=[144, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%410, %trunk._blocks.2._se_expand.weight, %trunk._blocks.2._se_expand.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
      "  %412 : Float(6, 144, 1, 1, strides=[144, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%411) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
      "  %413 : Float(6, 144, 32, 88, strides=[405504, 2816, 88, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%412, %406) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
      "  %683 : Float(6, 24, 32, 88, strides=[67584, 2816, 88, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%413, %684, %685)\n",
      "  %416 : Float(6, 24, 32, 88, strides=[67584, 2816, 88, 1], requires_grad=1, device=cuda:0) = onnx::Add(%683, %674) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:131:0\n",
      "  %686 : Float(6, 144, 32, 88, strides=[405504, 2816, 88, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%416, %687, %688)\n",
      "  %419 : Float(6, 144, 32, 88, strides=[405504, 2816, 88, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%686) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %420 : Float(6, 144, 32, 88, strides=[405504, 2816, 88, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%686, %419) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %421 : Float(6, 144, 36, 92, strides=[476928, 3312, 92, 1], requires_grad=1, device=cuda:0) = onnx::Pad[mode=\"constant\", pads=[0, 0, 2, 2, 0, 0, 2, 2], value=0.](%420) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:4174:0\n",
      "  %689 : Float(6, 144, 16, 44, strides=[101376, 704, 44, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=144, kernel_shape=[5, 5], pads=[0, 0, 0, 0], strides=[2, 2]](%421, %690, %691)\n",
      "  %424 : Float(6, 144, 16, 44, strides=[101376, 704, 44, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%689) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %425 : Float(6, 144, 16, 44, strides=[101376, 704, 44, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%689, %424) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %426 : Float(6, 144, 1, 1, strides=[144, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::GlobalAveragePool(%425) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1131:0\n",
      "  %427 : Float(6, 6, 1, 1, strides=[6, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%426, %trunk._blocks.3._se_reduce.weight, %trunk._blocks.3._se_reduce.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
      "  %428 : Float(6, 6, 1, 1, strides=[6, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%427) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %429 : Float(6, 6, 1, 1, strides=[6, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%427, %428) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %430 : Float(6, 144, 1, 1, strides=[144, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%429, %trunk._blocks.3._se_expand.weight, %trunk._blocks.3._se_expand.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
      "  %431 : Float(6, 144, 1, 1, strides=[144, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%430) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
      "  %432 : Float(6, 144, 16, 44, strides=[101376, 704, 44, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%431, %425) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
      "  %692 : Float(6, 40, 16, 44, strides=[28160, 704, 44, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%432, %693, %694)\n",
      "  %695 : Float(6, 240, 16, 44, strides=[168960, 704, 44, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%692, %696, %697)\n",
      "  %437 : Float(6, 240, 16, 44, strides=[168960, 704, 44, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%695) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %438 : Float(6, 240, 16, 44, strides=[168960, 704, 44, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%695, %437) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %439 : Float(6, 240, 20, 48, strides=[230400, 960, 48, 1], requires_grad=1, device=cuda:0) = onnx::Pad[mode=\"constant\", pads=[0, 0, 2, 2, 0, 0, 2, 2], value=0.](%438) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:4174:0\n",
      "  %698 : Float(6, 240, 16, 44, strides=[168960, 704, 44, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=240, kernel_shape=[5, 5], pads=[0, 0, 0, 0], strides=[1, 1]](%439, %699, %700)\n",
      "  %442 : Float(6, 240, 16, 44, strides=[168960, 704, 44, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%698) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %443 : Float(6, 240, 16, 44, strides=[168960, 704, 44, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%698, %442) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %444 : Float(6, 240, 1, 1, strides=[240, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::GlobalAveragePool(%443) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1131:0\n",
      "  %445 : Float(6, 10, 1, 1, strides=[10, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%444, %trunk._blocks.4._se_reduce.weight, %trunk._blocks.4._se_reduce.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
      "  %446 : Float(6, 10, 1, 1, strides=[10, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%445) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %447 : Float(6, 10, 1, 1, strides=[10, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%445, %446) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %448 : Float(6, 240, 1, 1, strides=[240, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%447, %trunk._blocks.4._se_expand.weight, %trunk._blocks.4._se_expand.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
      "  %449 : Float(6, 240, 1, 1, strides=[240, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%448) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
      "  %450 : Float(6, 240, 16, 44, strides=[168960, 704, 44, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%449, %443) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
      "  %701 : Float(6, 40, 16, 44, strides=[28160, 704, 44, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%450, %702, %703)\n",
      "  %453 : Float(6, 40, 16, 44, strides=[28160, 704, 44, 1], requires_grad=1, device=cuda:0) = onnx::Add(%701, %692) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:131:0\n",
      "  %704 : Float(6, 240, 16, 44, strides=[168960, 704, 44, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%453, %705, %706)\n",
      "  %456 : Float(6, 240, 16, 44, strides=[168960, 704, 44, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%704) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %457 : Float(6, 240, 16, 44, strides=[168960, 704, 44, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%704, %456) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %458 : Float(6, 240, 18, 46, strides=[198720, 828, 46, 1], requires_grad=1, device=cuda:0) = onnx::Pad[mode=\"constant\", pads=[0, 0, 1, 1, 0, 0, 1, 1], value=0.](%457) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:4174:0\n",
      "  %707 : Float(6, 240, 8, 22, strides=[42240, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=240, kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[2, 2]](%458, %708, %709)\n",
      "  %461 : Float(6, 240, 8, 22, strides=[42240, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%707) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %462 : Float(6, 240, 8, 22, strides=[42240, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%707, %461) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %463 : Float(6, 240, 1, 1, strides=[240, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::GlobalAveragePool(%462) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1131:0\n",
      "  %464 : Float(6, 10, 1, 1, strides=[10, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%463, %trunk._blocks.5._se_reduce.weight, %trunk._blocks.5._se_reduce.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
      "  %465 : Float(6, 10, 1, 1, strides=[10, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%464) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %466 : Float(6, 10, 1, 1, strides=[10, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%464, %465) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %467 : Float(6, 240, 1, 1, strides=[240, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%466, %trunk._blocks.5._se_expand.weight, %trunk._blocks.5._se_expand.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
      "  %468 : Float(6, 240, 1, 1, strides=[240, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%467) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
      "  %469 : Float(6, 240, 8, 22, strides=[42240, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%468, %462) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
      "  %710 : Float(6, 80, 8, 22, strides=[14080, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%469, %711, %712)\n",
      "  %713 : Float(6, 480, 8, 22, strides=[84480, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%710, %714, %715)\n",
      "  %474 : Float(6, 480, 8, 22, strides=[84480, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%713) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %475 : Float(6, 480, 8, 22, strides=[84480, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%713, %474) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %476 : Float(6, 480, 10, 24, strides=[115200, 240, 24, 1], requires_grad=1, device=cuda:0) = onnx::Pad[mode=\"constant\", pads=[0, 0, 1, 1, 0, 0, 1, 1], value=0.](%475) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:4174:0\n",
      "  %716 : Float(6, 480, 8, 22, strides=[84480, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=480, kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[1, 1]](%476, %717, %718)\n",
      "  %479 : Float(6, 480, 8, 22, strides=[84480, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%716) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %480 : Float(6, 480, 8, 22, strides=[84480, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%716, %479) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %481 : Float(6, 480, 1, 1, strides=[480, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::GlobalAveragePool(%480) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1131:0\n",
      "  %482 : Float(6, 20, 1, 1, strides=[20, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%481, %trunk._blocks.6._se_reduce.weight, %trunk._blocks.6._se_reduce.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
      "  %483 : Float(6, 20, 1, 1, strides=[20, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%482) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %484 : Float(6, 20, 1, 1, strides=[20, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%482, %483) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %485 : Float(6, 480, 1, 1, strides=[480, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%484, %trunk._blocks.6._se_expand.weight, %trunk._blocks.6._se_expand.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
      "  %486 : Float(6, 480, 1, 1, strides=[480, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%485) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
      "  %487 : Float(6, 480, 8, 22, strides=[84480, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%486, %480) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
      "  %719 : Float(6, 80, 8, 22, strides=[14080, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%487, %720, %721)\n",
      "  %490 : Float(6, 80, 8, 22, strides=[14080, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Add(%719, %710) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:131:0\n",
      "  %722 : Float(6, 480, 8, 22, strides=[84480, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%490, %723, %724)\n",
      "  %493 : Float(6, 480, 8, 22, strides=[84480, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%722) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %494 : Float(6, 480, 8, 22, strides=[84480, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%722, %493) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %495 : Float(6, 480, 10, 24, strides=[115200, 240, 24, 1], requires_grad=1, device=cuda:0) = onnx::Pad[mode=\"constant\", pads=[0, 0, 1, 1, 0, 0, 1, 1], value=0.](%494) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:4174:0\n",
      "  %725 : Float(6, 480, 8, 22, strides=[84480, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=480, kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[1, 1]](%495, %726, %727)\n",
      "  %498 : Float(6, 480, 8, 22, strides=[84480, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%725) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %499 : Float(6, 480, 8, 22, strides=[84480, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%725, %498) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %500 : Float(6, 480, 1, 1, strides=[480, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::GlobalAveragePool(%499) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1131:0\n",
      "  %501 : Float(6, 20, 1, 1, strides=[20, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%500, %trunk._blocks.7._se_reduce.weight, %trunk._blocks.7._se_reduce.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
      "  %502 : Float(6, 20, 1, 1, strides=[20, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%501) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %503 : Float(6, 20, 1, 1, strides=[20, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%501, %502) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %504 : Float(6, 480, 1, 1, strides=[480, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%503, %trunk._blocks.7._se_expand.weight, %trunk._blocks.7._se_expand.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
      "  %505 : Float(6, 480, 1, 1, strides=[480, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%504) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
      "  %506 : Float(6, 480, 8, 22, strides=[84480, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%505, %499) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
      "  %728 : Float(6, 80, 8, 22, strides=[14080, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%506, %729, %730)\n",
      "  %509 : Float(6, 80, 8, 22, strides=[14080, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Add(%728, %490) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:131:0\n",
      "  %731 : Float(6, 480, 8, 22, strides=[84480, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%509, %732, %733)\n",
      "  %512 : Float(6, 480, 8, 22, strides=[84480, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%731) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %513 : Float(6, 480, 8, 22, strides=[84480, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%731, %512) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %514 : Float(6, 480, 12, 26, strides=[149760, 312, 26, 1], requires_grad=1, device=cuda:0) = onnx::Pad[mode=\"constant\", pads=[0, 0, 2, 2, 0, 0, 2, 2], value=0.](%513) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:4174:0\n",
      "  %734 : Float(6, 480, 8, 22, strides=[84480, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=480, kernel_shape=[5, 5], pads=[0, 0, 0, 0], strides=[1, 1]](%514, %735, %736)\n",
      "  %517 : Float(6, 480, 8, 22, strides=[84480, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%734) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %518 : Float(6, 480, 8, 22, strides=[84480, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%734, %517) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %519 : Float(6, 480, 1, 1, strides=[480, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::GlobalAveragePool(%518) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1131:0\n",
      "  %520 : Float(6, 20, 1, 1, strides=[20, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%519, %trunk._blocks.8._se_reduce.weight, %trunk._blocks.8._se_reduce.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
      "  %521 : Float(6, 20, 1, 1, strides=[20, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%520) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %522 : Float(6, 20, 1, 1, strides=[20, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%520, %521) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %523 : Float(6, 480, 1, 1, strides=[480, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%522, %trunk._blocks.8._se_expand.weight, %trunk._blocks.8._se_expand.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
      "  %524 : Float(6, 480, 1, 1, strides=[480, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%523) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
      "  %525 : Float(6, 480, 8, 22, strides=[84480, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%524, %518) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
      "  %737 : Float(6, 112, 8, 22, strides=[19712, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%525, %738, %739)\n",
      "  %740 : Float(6, 672, 8, 22, strides=[118272, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%737, %741, %742)\n",
      "  %530 : Float(6, 672, 8, 22, strides=[118272, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%740) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %531 : Float(6, 672, 8, 22, strides=[118272, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%740, %530) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %532 : Float(6, 672, 12, 26, strides=[209664, 312, 26, 1], requires_grad=1, device=cuda:0) = onnx::Pad[mode=\"constant\", pads=[0, 0, 2, 2, 0, 0, 2, 2], value=0.](%531) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:4174:0\n",
      "  %743 : Float(6, 672, 8, 22, strides=[118272, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=672, kernel_shape=[5, 5], pads=[0, 0, 0, 0], strides=[1, 1]](%532, %744, %745)\n",
      "  %535 : Float(6, 672, 8, 22, strides=[118272, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%743) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %536 : Float(6, 672, 8, 22, strides=[118272, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%743, %535) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %537 : Float(6, 672, 1, 1, strides=[672, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::GlobalAveragePool(%536) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1131:0\n",
      "  %538 : Float(6, 28, 1, 1, strides=[28, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%537, %trunk._blocks.9._se_reduce.weight, %trunk._blocks.9._se_reduce.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
      "  %539 : Float(6, 28, 1, 1, strides=[28, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%538) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %540 : Float(6, 28, 1, 1, strides=[28, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%538, %539) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %541 : Float(6, 672, 1, 1, strides=[672, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%540, %trunk._blocks.9._se_expand.weight, %trunk._blocks.9._se_expand.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
      "  %542 : Float(6, 672, 1, 1, strides=[672, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%541) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
      "  %543 : Float(6, 672, 8, 22, strides=[118272, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%542, %536) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
      "  %746 : Float(6, 112, 8, 22, strides=[19712, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%543, %747, %748)\n",
      "  %546 : Float(6, 112, 8, 22, strides=[19712, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Add(%746, %737) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:131:0\n",
      "  %749 : Float(6, 672, 8, 22, strides=[118272, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%546, %750, %751)\n",
      "  %549 : Float(6, 672, 8, 22, strides=[118272, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%749) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %550 : Float(6, 672, 8, 22, strides=[118272, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%749, %549) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %551 : Float(6, 672, 12, 26, strides=[209664, 312, 26, 1], requires_grad=1, device=cuda:0) = onnx::Pad[mode=\"constant\", pads=[0, 0, 2, 2, 0, 0, 2, 2], value=0.](%550) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:4174:0\n",
      "  %752 : Float(6, 672, 8, 22, strides=[118272, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=672, kernel_shape=[5, 5], pads=[0, 0, 0, 0], strides=[1, 1]](%551, %753, %754)\n",
      "  %554 : Float(6, 672, 8, 22, strides=[118272, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%752) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %555 : Float(6, 672, 8, 22, strides=[118272, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%752, %554) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %556 : Float(6, 672, 1, 1, strides=[672, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::GlobalAveragePool(%555) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1131:0\n",
      "  %557 : Float(6, 28, 1, 1, strides=[28, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%556, %trunk._blocks.10._se_reduce.weight, %trunk._blocks.10._se_reduce.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
      "  %558 : Float(6, 28, 1, 1, strides=[28, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%557) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %559 : Float(6, 28, 1, 1, strides=[28, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%557, %558) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %560 : Float(6, 672, 1, 1, strides=[672, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%559, %trunk._blocks.10._se_expand.weight, %trunk._blocks.10._se_expand.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
      "  %561 : Float(6, 672, 1, 1, strides=[672, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%560) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
      "  %562 : Float(6, 672, 8, 22, strides=[118272, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%561, %555) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
      "  %755 : Float(6, 112, 8, 22, strides=[19712, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%562, %756, %757)\n",
      "  %565 : Float(6, 112, 8, 22, strides=[19712, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Add(%755, %546) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:131:0\n",
      "  %758 : Float(6, 672, 8, 22, strides=[118272, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%565, %759, %760)\n",
      "  %568 : Float(6, 672, 8, 22, strides=[118272, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%758) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %569 : Float(6, 672, 8, 22, strides=[118272, 176, 22, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%758, %568) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %570 : Float(6, 672, 12, 26, strides=[209664, 312, 26, 1], requires_grad=1, device=cuda:0) = onnx::Pad[mode=\"constant\", pads=[0, 0, 2, 2, 0, 0, 2, 2], value=0.](%569) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:4174:0\n",
      "  %761 : Float(6, 672, 4, 11, strides=[29568, 44, 11, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=672, kernel_shape=[5, 5], pads=[0, 0, 0, 0], strides=[2, 2]](%570, %762, %763)\n",
      "  %573 : Float(6, 672, 4, 11, strides=[29568, 44, 11, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%761) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %574 : Float(6, 672, 4, 11, strides=[29568, 44, 11, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%761, %573) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %575 : Float(6, 672, 1, 1, strides=[672, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::GlobalAveragePool(%574) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1131:0\n",
      "  %576 : Float(6, 28, 1, 1, strides=[28, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%575, %trunk._blocks.11._se_reduce.weight, %trunk._blocks.11._se_reduce.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
      "  %577 : Float(6, 28, 1, 1, strides=[28, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%576) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %578 : Float(6, 28, 1, 1, strides=[28, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%576, %577) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %579 : Float(6, 672, 1, 1, strides=[672, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%578, %trunk._blocks.11._se_expand.weight, %trunk._blocks.11._se_expand.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
      "  %580 : Float(6, 672, 1, 1, strides=[672, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%579) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
      "  %581 : Float(6, 672, 4, 11, strides=[29568, 44, 11, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%580, %574) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
      "  %764 : Float(6, 192, 4, 11, strides=[8448, 44, 11, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%581, %765, %766)\n",
      "  %767 : Float(6, 1152, 4, 11, strides=[50688, 44, 11, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%764, %768, %769)\n",
      "  %586 : Float(6, 1152, 4, 11, strides=[50688, 44, 11, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%767) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %587 : Float(6, 1152, 4, 11, strides=[50688, 44, 11, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%767, %586) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %588 : Float(6, 1152, 8, 15, strides=[138240, 120, 15, 1], requires_grad=1, device=cuda:0) = onnx::Pad[mode=\"constant\", pads=[0, 0, 2, 2, 0, 0, 2, 2], value=0.](%587) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:4174:0\n",
      "  %770 : Float(6, 1152, 4, 11, strides=[50688, 44, 11, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1152, kernel_shape=[5, 5], pads=[0, 0, 0, 0], strides=[1, 1]](%588, %771, %772)\n",
      "  %591 : Float(6, 1152, 4, 11, strides=[50688, 44, 11, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%770) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %592 : Float(6, 1152, 4, 11, strides=[50688, 44, 11, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%770, %591) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %593 : Float(6, 1152, 1, 1, strides=[1152, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::GlobalAveragePool(%592) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1131:0\n",
      "  %594 : Float(6, 48, 1, 1, strides=[48, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%593, %trunk._blocks.12._se_reduce.weight, %trunk._blocks.12._se_reduce.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
      "  %595 : Float(6, 48, 1, 1, strides=[48, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%594) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %596 : Float(6, 48, 1, 1, strides=[48, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%594, %595) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %597 : Float(6, 1152, 1, 1, strides=[1152, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%596, %trunk._blocks.12._se_expand.weight, %trunk._blocks.12._se_expand.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
      "  %598 : Float(6, 1152, 1, 1, strides=[1152, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%597) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
      "  %599 : Float(6, 1152, 4, 11, strides=[50688, 44, 11, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%598, %592) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
      "  %773 : Float(6, 192, 4, 11, strides=[8448, 44, 11, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%599, %774, %775)\n",
      "  %602 : Float(6, 192, 4, 11, strides=[8448, 44, 11, 1], requires_grad=1, device=cuda:0) = onnx::Add(%773, %764) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:131:0\n",
      "  %776 : Float(6, 1152, 4, 11, strides=[50688, 44, 11, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%602, %777, %778)\n",
      "  %605 : Float(6, 1152, 4, 11, strides=[50688, 44, 11, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%776) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %606 : Float(6, 1152, 4, 11, strides=[50688, 44, 11, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%776, %605) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %607 : Float(6, 1152, 8, 15, strides=[138240, 120, 15, 1], requires_grad=1, device=cuda:0) = onnx::Pad[mode=\"constant\", pads=[0, 0, 2, 2, 0, 0, 2, 2], value=0.](%606) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:4174:0\n",
      "  %779 : Float(6, 1152, 4, 11, strides=[50688, 44, 11, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1152, kernel_shape=[5, 5], pads=[0, 0, 0, 0], strides=[1, 1]](%607, %780, %781)\n",
      "  %610 : Float(6, 1152, 4, 11, strides=[50688, 44, 11, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%779) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %611 : Float(6, 1152, 4, 11, strides=[50688, 44, 11, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%779, %610) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %612 : Float(6, 1152, 1, 1, strides=[1152, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::GlobalAveragePool(%611) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1131:0\n",
      "  %613 : Float(6, 48, 1, 1, strides=[48, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%612, %trunk._blocks.13._se_reduce.weight, %trunk._blocks.13._se_reduce.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
      "  %614 : Float(6, 48, 1, 1, strides=[48, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%613) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %615 : Float(6, 48, 1, 1, strides=[48, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%613, %614) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %616 : Float(6, 1152, 1, 1, strides=[1152, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%615, %trunk._blocks.13._se_expand.weight, %trunk._blocks.13._se_expand.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
      "  %617 : Float(6, 1152, 1, 1, strides=[1152, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%616) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
      "  %618 : Float(6, 1152, 4, 11, strides=[50688, 44, 11, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%617, %611) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
      "  %782 : Float(6, 192, 4, 11, strides=[8448, 44, 11, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%618, %783, %784)\n",
      "  %621 : Float(6, 192, 4, 11, strides=[8448, 44, 11, 1], requires_grad=1, device=cuda:0) = onnx::Add(%782, %602) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:131:0\n",
      "  %785 : Float(6, 1152, 4, 11, strides=[50688, 44, 11, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%621, %786, %787)\n",
      "  %624 : Float(6, 1152, 4, 11, strides=[50688, 44, 11, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%785) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %625 : Float(6, 1152, 4, 11, strides=[50688, 44, 11, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%785, %624) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %626 : Float(6, 1152, 8, 15, strides=[138240, 120, 15, 1], requires_grad=1, device=cuda:0) = onnx::Pad[mode=\"constant\", pads=[0, 0, 2, 2, 0, 0, 2, 2], value=0.](%625) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:4174:0\n",
      "  %788 : Float(6, 1152, 4, 11, strides=[50688, 44, 11, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1152, kernel_shape=[5, 5], pads=[0, 0, 0, 0], strides=[1, 1]](%626, %789, %790)\n",
      "  %629 : Float(6, 1152, 4, 11, strides=[50688, 44, 11, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%788) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %630 : Float(6, 1152, 4, 11, strides=[50688, 44, 11, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%788, %629) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %631 : Float(6, 1152, 1, 1, strides=[1152, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::GlobalAveragePool(%630) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1131:0\n",
      "  %632 : Float(6, 48, 1, 1, strides=[48, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%631, %trunk._blocks.14._se_reduce.weight, %trunk._blocks.14._se_reduce.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
      "  %633 : Float(6, 48, 1, 1, strides=[48, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%632) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %634 : Float(6, 48, 1, 1, strides=[48, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%632, %633) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %635 : Float(6, 1152, 1, 1, strides=[1152, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%634, %trunk._blocks.14._se_expand.weight, %trunk._blocks.14._se_expand.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
      "  %636 : Float(6, 1152, 1, 1, strides=[1152, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%635) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
      "  %637 : Float(6, 1152, 4, 11, strides=[50688, 44, 11, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%636, %630) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
      "  %791 : Float(6, 192, 4, 11, strides=[8448, 44, 11, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%637, %792, %793)\n",
      "  %640 : Float(6, 192, 4, 11, strides=[8448, 44, 11, 1], requires_grad=1, device=cuda:0) = onnx::Add(%791, %621) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:131:0\n",
      "  %794 : Float(6, 1152, 4, 11, strides=[50688, 44, 11, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%640, %795, %796)\n",
      "  %643 : Float(6, 1152, 4, 11, strides=[50688, 44, 11, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%794) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %644 : Float(6, 1152, 4, 11, strides=[50688, 44, 11, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%794, %643) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %645 : Float(6, 1152, 6, 13, strides=[89856, 78, 13, 1], requires_grad=1, device=cuda:0) = onnx::Pad[mode=\"constant\", pads=[0, 0, 1, 1, 0, 0, 1, 1], value=0.](%644) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:4174:0\n",
      "  %797 : Float(6, 1152, 4, 11, strides=[50688, 44, 11, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1152, kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[1, 1]](%645, %798, %799)\n",
      "  %648 : Float(6, 1152, 4, 11, strides=[50688, 44, 11, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%797) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %649 : Float(6, 1152, 4, 11, strides=[50688, 44, 11, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%797, %648) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %650 : Float(6, 1152, 1, 1, strides=[1152, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::GlobalAveragePool(%649) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1131:0\n",
      "  %651 : Float(6, 48, 1, 1, strides=[48, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%650, %trunk._blocks.15._se_reduce.weight, %trunk._blocks.15._se_reduce.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
      "  %652 : Float(6, 48, 1, 1, strides=[48, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%651) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %653 : Float(6, 48, 1, 1, strides=[48, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%651, %652) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:57:0\n",
      "  %654 : Float(6, 1152, 1, 1, strides=[1152, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%653, %trunk._blocks.15._se_expand.weight, %trunk._blocks.15._se_expand.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
      "  %655 : Float(6, 1152, 1, 1, strides=[1152, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Sigmoid(%654) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
      "  %656 : Float(6, 1152, 4, 11, strides=[50688, 44, 11, 1], requires_grad=1, device=cuda:0) = onnx::Mul(%655, %649) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
      "  %output : Float(6, 320, 4, 11, strides=[14080, 44, 11, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%656, %801, %802)\n",
      "  return (%output)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ONNX_PATH='models/reduction_5_model.onnx'\n",
    "torch.onnx.export(reduction_5_model, dummy_input1, ONNX_PATH,\n",
    "                    export_params=True,\n",
    "                    verbose=True, \n",
    "                    input_names=['input'],\n",
    "                    output_names=['output'],\n",
    "\n",
    "                    opset_version=10,\n",
    "                    do_constant_folding=False,\n",
    "                    #example_outputs=torch_out,\n",
    "                    #strip_doc_string=True,\n",
    "                    #dynamic_axes=None,\n",
    "                    keep_initializers_as_inputs=True\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 112, 8, 22])\n",
      "torch.Size([6, 320, 4, 11])\n",
      "Model output = 0.0\n",
      "Model output = 0.0\n"
     ]
    }
   ],
   "source": [
    "y_pred4 = reduction_4_model(dummy_input1)\n",
    "\n",
    "y_pred5 = reduction_5_model(dummy_input1)\n",
    "print(y_pred4.shape)\n",
    "print(y_pred5.shape)\n",
    "print(f\"Model output = {torch.norm(reduction_4_model(dummy_input1) - reduction_4_model(dummy_input1))}\")\n",
    "print(f\"Model output = {torch.norm(reduction_5_model(dummy_input1) - reduction_5_model(dummy_input1))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the inference with onnx runtime "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rtml/.local/lib/python3.6/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:56: UserWarning: Specified provider 'CUDAExecutionProvider' is not in available provider names.Available providers: 'CPUExecutionProvider'\n",
      "  \"Available providers: '{}'\".format(name, \", \".join(available_provider_names)))\n"
     ]
    }
   ],
   "source": [
    "ort_session = onnxrt.InferenceSession('models/reduction_4_model.onnx', None, providers=[\"CUDAExecutionProvider\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 112, 8, 22)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "\nNot equal to tolerance rtol=0.001, atol=1e-05\n\nMismatched elements: 78 / 118272 (0.0659%)\nMax absolute difference: 0.00011063\nMax relative difference: 0.46363637\n x: array([[[[-17.735939,  -0.434797,   8.992467, ...,  11.1043  ,\n           12.302662,  10.307653],\n         [ -0.566915,  -7.637162,  -2.469571, ...,   0.45895 ,...\n y: array([[[[-17.735909,  -0.434755,   8.9925  , ...,  11.104317,\n           12.302671,  10.307664],\n         [ -0.566899,  -7.637188,  -2.469585, ...,   0.458945,...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-118bef7d5217>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mpred_onx4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mort_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdummy_input1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_onx4\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtesting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_allclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_onx4\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-03\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: \nNot equal to tolerance rtol=0.001, atol=1e-05\n\nMismatched elements: 78 / 118272 (0.0659%)\nMax absolute difference: 0.00011063\nMax relative difference: 0.46363637\n x: array([[[[-17.735939,  -0.434797,   8.992467, ...,  11.1043  ,\n           12.302662,  10.307653],\n         [ -0.566915,  -7.637162,  -2.469571, ...,   0.45895 ,...\n y: array([[[[-17.735909,  -0.434755,   8.9925  , ...,  11.104317,\n           12.302671,  10.307664],\n         [ -0.566899,  -7.637188,  -2.469585, ...,   0.458945,..."
     ]
    }
   ],
   "source": [
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "inputs = [node for node in ort_session.get_inputs()][0]\n",
    "outputs = [node.name for node in ort_session.get_outputs()]\n",
    "\n",
    "pred_onx4 = ort_session.run(outputs, {inputs.name: to_numpy(dummy_input1)})\n",
    "print(pred_onx4[0].shape)\n",
    "np.testing.assert_allclose(to_numpy(y_pred4.cpu()), pred_onx4[0], rtol=1e-03, atol=1e-05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0053324476"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(to_numpy(y_pred4.cpu())-pred_onx4[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_session = onnxrt.InferenceSession('models/reduction_5_model.onnx', None, providers=[\"CUDAExecutionProvider\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 320, 4, 11)\n"
     ]
    }
   ],
   "source": [
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "inputs = [node for node in ort_session.get_inputs()][0]\n",
    "outputs = [node.name for node in ort_session.get_outputs()]\n",
    "\n",
    "pred_onx5 = ort_session.run(outputs, {inputs.name: to_numpy(dummy_input1)})\n",
    "print(pred_onx5[0].shape)\n",
    "np.testing.assert_allclose(to_numpy(y_pred5.cpu()), pred_onx5[0], rtol=1e-03, atol=1e-05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0016395646"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(to_numpy(y_pred5.cpu())-pred_onx5[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking with onnx simplifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load your predefined ONNX model\n",
    "model = onnx.load('models/camenc_b0.onnx')\n",
    "\n",
    "# convert model\n",
    "model_simp, check = simplify(model)\n",
    "\n",
    "assert check, \"Simplified ONNX model could not be validated\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx.save(model_simp, 'models/simp_camenc_b0.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load your predefined ONNX model\n",
    "model = onnx.load('models/camenc_b0_1.onnx')\n",
    "\n",
    "# convert model\n",
    "model_simp, check = simplify(model)\n",
    "\n",
    "assert check, \"Simplified ONNX model could not be validated\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx.save(model_simp, 'models/simp_camenc_b0_1.onnx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
